{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **QUESTION 1**","metadata":{}},{"cell_type":"markdown","source":"## **DATASET LOADING**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom sklearn.datasets import make_classification\n\n\n# # Load the PIMA Indians diabetes dataset\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\ncolumns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n           'DiabetesPedigreeFunction', 'Age', 'Outcome']\n\n# # Load the dataset into a DataFrame\ndata = pd.read_csv(url, names=columns)","metadata":{"executionInfo":{"elapsed":6176,"status":"ok","timestamp":1725880132018,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"irtwvlBvwiFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1725880132019,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"GwlxST6H2MLK","outputId":"eaeebf24-ed53-4b0c-ab51-8745ce037999"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"# Replace zero values with NaN for relevant columns\n# data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)\n\n# Impute missing values using the median\nimputer = SimpleImputer(strategy='median')\ndata_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n\n# Define the function to replace outliers with the median\ndef replace_outliers_with_median(err_arr):\n    a = err_arr\n    med = np.median(a)\n    outlierConstant = 1.5\n    upper_quartile = np.percentile(a, 75)\n    lower_quartile = np.percentile(a, 25)\n    IQR = (upper_quartile - lower_quartile) * outlierConstant\n    quartileSet = (lower_quartile - IQR, upper_quartile + IQR)\n    # Find the outliers within 75% interval and replace them with median value\n    output = np.where((a >= quartileSet[0]) & (a <= quartileSet[1]), a, med)\n    return output\n\n# Apply the outlier replacement with median on relevant features\nfor column in ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age']:\n    data_imputed[column] = replace_outliers_with_median(data_imputed[column])\n\n# Log transform skewed features (optional)\n# for column in ['Insulin', 'SkinThickness', 'BMI']:\n#     data_imputed[column] = np.log1p(data_imputed[column])\n\n# Standardize the feature set\nscaler = StandardScaler()\nX = data_imputed.drop('Outcome', axis=1)\nX_scaled = scaler.fit_transform(X)\n\n# Outcome (target)\ny = data_imputed['Outcome']\n\n# Apply SMOTE to the entire dataset before splitting\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n\n# Split the resampled data into training and testing sets (70% training, 30% testing)\nX_train_resampled, X_test_resampled, y_train_resampled, y_test_resampled = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# # Split into training and testing sets (70% training, 30% testing) before resampling\n# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# # Apply SMOTE only to the training set to handle class imbalance\n# smote = SMOTE(random_state=42)\n# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)","metadata":{"executionInfo":{"elapsed":345,"status":"ok","timestamp":1725882338948,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"7y9r066QzPHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **HYPERPARAMETER TUNING**","metadata":{}},{"cell_type":"markdown","source":"### **DECISION TREE CLASSIFIER**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Create the model\ndt_model = DecisionTreeClassifier(random_state=42)\n\n# Define the hyperparameter space without 'auto'\nparam_dist_dt = {\n    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n    'splitter': ['best', 'random'],  # Strategy used to split at each node\n    'max_depth': np.arange(3, 20),  # Maximum depth of the tree\n    'min_samples_split': np.arange(2, 20),  # Minimum number of samples required to split an internal node\n    'min_samples_leaf': np.arange(1, 10),  # Minimum number of samples required to be at a leaf node\n    'max_features': ['sqrt', 'log2', None],  # Valid options for the number of features to consider for the best split\n}\n\n# Define multiple scoring metrics\nscoring = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score, average='weighted'),\n    'recall': make_scorer(recall_score, average='weighted'),\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Perform the random search as before\nrandom_search_dt = RandomizedSearchCV(dt_model, param_distributions=param_dist_dt, n_iter=50, scoring=scoring, refit='accuracy', cv=5, random_state=42, n_jobs=-1)\n\n# Fit the model\nrandom_search_dt.fit(X_resampled, y_resampled)\n\n# Extract and sort the results by accuracy\nresults_dt = sorted(\n    zip(\n        random_search_dt.cv_results_['params'],\n        random_search_dt.cv_results_['mean_test_accuracy'],\n        random_search_dt.cv_results_['mean_test_precision'],\n        random_search_dt.cv_results_['mean_test_recall'],\n        random_search_dt.cv_results_['mean_test_f1']\n    ),\n    key=lambda x: x[1]  # Sort by accuracy (index 1 in the tuple)\n)\n\n# Print the results as before\nprint(\"Decision Tree: Hyperparameter combinations and corresponding metrics (ordered by accuracy):\")\nfor i, (params, accuracy, precision, recall, f1) in enumerate(results_dt):\n    print(f\"Case {i + 1}: {params}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1:.4f}\\n\")\n\n# Print the best combination of hyperparameters and its score\nprint(\"\\nBest Decision Tree hyperparameters:\")\nprint(random_search_dt.best_params_)\nprint(f\"Best accuracy: {random_search_dt.best_score_:.4f}\")\n","metadata":{"executionInfo":{"elapsed":11521,"status":"ok","timestamp":1725884522928,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"uNpjwI7Sk6Hw","outputId":"7c0502c7-5c09-4e9b-efa8-8740cd7a3a26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **RANDOM FOREST CLASSIFIER**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Create the model\nrf_model = RandomForestClassifier(random_state=42)\n\n# Define the hyperparameter space\nparam_dist = {\n    'n_estimators': [100],  # Number of trees in the forest\n    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the tree\n    'min_samples_split': np.arange(2, 10),  # Minimum number of samples required to split a node\n    'min_samples_leaf': np.arange(1, 5),  # Minimum number of samples required at a leaf node\n    'max_features': ['sqrt', 'log2', None],  # Updated to remove 'auto'\n    'bootstrap': [True, False]  # Whether to bootstrap samples when building trees\n}\n\n# Define multiple scoring metrics\nscoring = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score, average='weighted'),\n    'recall': make_scorer(recall_score, average='weighted'),\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Randomized search with 50 different combinations of hyperparameters\nrandom_search = RandomizedSearchCV(\n    rf_model,\n    param_distributions=param_dist,\n    n_iter=50,\n    scoring=scoring,\n    refit='accuracy',\n    cv=5,\n    random_state=42,\n    n_jobs=-1,\n    error_score='raise'  # Optional: set to 'raise' to debug further issues\n)\n\n# Fit the random search model\nrandom_search.fit(X_resampled, y_resampled)  # Ensure X_resampled and y_resampled are defined\n\n# Extract and sort the results by accuracy\nresults = sorted(\n    zip(\n        random_search.cv_results_['params'],\n        random_search.cv_results_['mean_test_accuracy'],\n        random_search.cv_results_['mean_test_precision'],\n        random_search.cv_results_['mean_test_recall'],\n        random_search.cv_results_['mean_test_f1']\n    ),\n    key=lambda x: x[1],  # Sort by accuracy (index 1 in the tuple)\n    reverse=True  # Optional: Sort in descending order for highest accuracy first\n)\n\n# Print out the hyperparameters and corresponding metrics for each case, ordered by accuracy\nprint(\"Hyperparameter combinations and corresponding metrics (ordered by accuracy):\")\nfor i, (params, accuracy, precision, recall, f1) in enumerate(results):\n    print(f\"Case {i + 1}: {params}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1:.4f}\\n\")\n\n# Print the best combination of hyperparameters and its score\nprint(\"\\nBest hyperparameters:\")\nprint(random_search.best_params_)\nprint(f\"Best accuracy: {random_search.best_score_:.4f}\")","metadata":{"executionInfo":{"elapsed":59733,"status":"ok","timestamp":1725885412008,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"XHpo_4NTwC1F","outputId":"21c063b1-f2b3-412c-9fef-0369dfb49f49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SUPPORT VECTOR MACHINE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **SUPPORT VECTOR MACHINE**","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Create the model\nsvm_model = SVC(random_state=42)\n\n# Define the hyperparameter space\nparam_dist = {\n    'C': np.logspace(-3, 2, 6),  # Regularization parameter\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel types\n    'degree': np.arange(2, 5),  # Degree for 'poly' kernel\n    'gamma': ['scale', 'auto'],  # Kernel coefficient\n    'coef0': [0.0, 0.1, 0.5],  # Independent term in kernel function for 'poly' and 'sigmoid'\n}\n\n# Define multiple scoring metrics\nscoring = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score, average='weighted'),\n    'recall': make_scorer(recall_score, average='weighted'),\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Randomized search with 100 different combinations of hyperparameters\nrandom_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=20, scoring=scoring, refit='accuracy', cv=5, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nrandom_search.fit(X_resampled, y_resampled)  # Assuming X_train_resampled, y_train_resampled are your training data\n\n# Extract and sort the results by accuracy\nresults = sorted(\n    zip(\n        random_search.cv_results_['params'],\n        random_search.cv_results_['mean_test_accuracy'],\n        random_search.cv_results_['mean_test_precision'],\n        random_search.cv_results_['mean_test_recall'],\n        random_search.cv_results_['mean_test_f1']\n    ),\n    key=lambda x: x[1]  # Sort by accuracy (index 1 in the tuple)\n)\n\n# Print out the hyperparameters and corresponding metrics for each case, ordered by accuracy\nprint(\"Hyperparameter combinations and corresponding metrics (ordered by accuracy):\")\nfor i, (params, accuracy, precision, recall, f1) in enumerate(results):\n    print(f\"Case {i + 1}: {params}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1:.4f}\\n\")\n\n# Print the best combination of hyperparameters and its score\nprint(\"\\nBest hyperparameters:\")\nprint(random_search.best_params_)\nprint(f\"Best accuracy: {random_search.best_score_:.4f}\")\n","metadata":{"executionInfo":{"elapsed":17984,"status":"ok","timestamp":1725884204464,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"VQiMp4EdsL-F","outputId":"b9f1f81c-fdc5-4feb-ad3c-2d893565a066"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **STACKING ENSEMBLE**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\n\n# Define the base estimators\nestimators = [\n    ('dt', DecisionTreeClassifier(random_state=42)),\n    ('rf', RandomForestClassifier(random_state=42)),\n    ('svm', SVC(random_state=42, probability=True))\n]\n\n# Define the stacking model with Logistic Regression as the final estimator\nstacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5)\n\n# Define the hyperparameter space for RandomizedSearchCV\nparam_dist = {\n    'final_estimator__C': np.logspace(-3, 2, 6),  # Regularization parameter for LogisticRegression\n    'rf__n_estimators': [50, 100, 200],  # Number of trees for RandomForest\n    'rf__max_depth': [10, 20, 30],  # Max depth of trees for RandomForest\n    'svm__C': np.logspace(-3, 2, 6),  # Regularization parameter for SVC\n    'dt__max_depth': [None, 10, 20, 30],  # Max depth for DecisionTree\n}\n\n# Define multiple scoring metrics\nscoring = {\n    'accuracy': make_scorer(accuracy_score),\n    'precision': make_scorer(precision_score, average='weighted'),\n    'recall': make_scorer(recall_score, average='weighted'),\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Randomized search with 20 different combinations of hyperparameters\nrandom_search = RandomizedSearchCV(stacking_model, param_distributions=param_dist, n_iter=10, scoring=scoring, refit='accuracy', cv=5, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nrandom_search.fit(X_resampled, y_resampled)  # Assuming X_resampled and y_resampled are your training data\n\n# Extract and sort the results by accuracy\nresults = sorted(\n    zip(\n        random_search.cv_results_['params'],\n        random_search.cv_results_['mean_test_accuracy'],\n        random_search.cv_results_['mean_test_precision'],\n        random_search.cv_results_['mean_test_recall'],\n        random_search.cv_results_['mean_test_f1']\n    ),\n    key=lambda x: x[1]  # Sort by accuracy (index 1 in the tuple)\n)\n\n# Print out the hyperparameters and corresponding metrics for each case, ordered by accuracy\nprint(\"Hyperparameter combinations and corresponding metrics (ordered by accuracy):\")\nfor i, (params, accuracy, precision, recall, f1) in enumerate(results):\n    print(f\"Case {i + 1}: {params}\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-score: {f1:.4f}\\n\")\n\n# Print the best combination of hyperparameters and its score\nprint(\"\\nBest hyperparameters:\")\nprint(random_search.best_params_)\nprint(f\"Best accuracy: {random_search.best_score_:.4f}\")\n","metadata":{"executionInfo":{"elapsed":164817,"status":"ok","timestamp":1725885773665,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"PzIIAp_EzEVx","outputId":"d52adda8-ca93-4aaf-8417-05e3133beaad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **TRAIN-TEST**","metadata":{"id":"RoIaYebo3--H"}},{"cell_type":"code","source":"# Models\ndt_model = DecisionTreeClassifier(splitter='random', min_samples_split=16, min_samples_leaf=2, max_features='sqrt', max_depth=4, criterion='gini', random_state=42)\nsvm_model = SVC(kernel='sigmoid', gamma='scale', degree=2, coef0=0.0, C=0.01, random_state=42)\nrf_model = RandomForestClassifier(n_estimators=100, min_samples_split=7, min_samples_leaf=1, max_features='sqrt', max_depth=10, bootstrap=True, random_state=42)","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1725885443809,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"bZNrhaZfw3fs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the base estimators with updated hyperparameters\nestimators = [\n    ('dt', DecisionTreeClassifier(max_depth=30, random_state=42)),  # Updated max_depth for Decision Tree\n    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),  # Updated n_estimators and max_depth for Random Forest\n    ('svm', SVC(C=1.0, probability=True, random_state=42))  # Updated C for SVM\n]\n\n# Define the stacking model with Logistic Regression as the final estimator with updated hyperparameter\nstacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(C=0.1), cv=5)  # Updated C for final estimator","metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1725886065331,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"2FODtWzPG894"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import Pipeline  # Use the regular sklearn pipeline here\nfrom imblearn.pipeline import Pipeline as imbPipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Assuming data_imputed is already loaded\nX = data_imputed.drop('Outcome', axis=1)\ny = data_imputed['Outcome']\n\n# Custom Transformer for IQR-based outlier handling\nclass IQRTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='median'):\n        self.strategy = strategy\n        self.medians = None\n\n    def fit(self, X, y=None):\n        # Calculate the IQR and medians for each feature\n        Q1 = np.percentile(X, 25, axis=0)\n        Q3 = np.percentile(X, 75, axis=0)\n        IQR = Q3 - Q1\n        self.lower_bound = Q1 - 1.5 * IQR\n        self.upper_bound = Q3 + 1.5 * IQR\n        self.medians = np.median(X, axis=0)\n        return self\n\n    def transform(self, X):\n        # Replace outliers with the median\n        X_transformed = np.where((X < self.lower_bound) | (X > self.upper_bound), self.medians, X)\n        return X_transformed\n\n# Define the preprocessing pipeline (without SMOTE for now)\npreprocessing_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Median imputation for missing values\n    ('iqr', IQRTransformer()),                     # IQR outlier handling\n    ('scaler', StandardScaler()),                   # Standard scaling\n])\n\n# Apply preprocessing to training and test sets\nX_preprocessed = preprocessing_pipeline.fit_transform(X)\nX_preprocessed = preprocessing_pipeline.transform(X)  # Apply the same transformations to the test set\n\n# Apply SMOTE only to the training data (after preprocessing)\nsmote = SMOTE(random_state=24)\nX_smote, y_smote = smote.fit_resample(X_preprocessed, y)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42, stratify=y_smote)\n\n# Define models\ndt_model = DecisionTreeClassifier(splitter='random', min_samples_split=15, min_samples_leaf=8, max_features='sqrt', max_depth=4, criterion='gini', random_state=42)\nrf_model = RandomForestClassifier(n_estimators=100, min_samples_split=6, min_samples_leaf=4, max_features='sqrt', max_depth=20, bootstrap=True, random_state=42)\nsvm_model = SVC(kernel='sigmoid', gamma='scale', degree=2, coef0=0.0, C=0.01, random_state=42)\n\n# Define the base estimators with the updated hyperparameters\nestimators = [\n    ('dt', DecisionTreeClassifier(max_depth=30, random_state=42)),  # Updated max_depth for Decision Tree\n    ('rf', RandomForestClassifier(n_estimators=50, max_depth=20, random_state=42)),  # Updated n_estimators and max_depth for Random Forest\n    ('svm', SVC(C=0.001, probability=True, random_state=42))  # Updated C for SVM\n]\n\n# Stacking model with updated final_estimator\nstacking_model = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression(C=0.01),  # Updated C for Logistic Regression final estimator\n    cv=5\n)\n\n# Fit and evaluate each model\n\n# Decision Tree\ndt_model.fit(X_train, y_train)\ny_pred_dt = dt_model.predict(X_test)\nprint(\"Decision Tree Train-Test Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.2f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_dt):.2f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_dt):.2f}\")\nprint(f\"F1-Score: {f1_score(y_test, y_pred_dt):.2f}\\n\")\n\n# Random Forest\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\nprint(\"Random Forest Train-Test Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_rf):.2f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_rf):.2f}\")\nprint(f\"F1-Score: {f1_score(y_test, y_pred_rf):.2f}\\n\")\n\n# SVM\nsvm_model.fit(X_train, y_train)\ny_pred_svm = svm_model.predict(X_test)\nprint(\"SVM Train-Test Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_svm):.2f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_svm):.2f}\")\nprint(f\"F1-Score: {f1_score(y_test, y_pred_svm):.2f}\\n\")\n\n# Stacking model\nstacking_model.fit(X_train, y_train)\ny_pred_stacking = stacking_model.predict(X_test)\nprint(\"Stacking Model Train-Test Results:\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_stacking):.2f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred_stacking):.2f}\")\nprint(f\"Recall: {recall_score(y_test, y_pred_stacking):.2f}\")\nprint(f\"F1-Score: {f1_score(y_test, y_pred_stacking):.2f}\\n\")\n\n# Decision Tree results\ndt_accuracy = accuracy_score(y_test, y_pred_dt)\ndt_precision = precision_score(y_test, y_pred_dt)\ndt_recall = recall_score(y_test, y_pred_dt)\ndt_f1 = f1_score(y_test, y_pred_dt)\ndt_results = [dt_accuracy, dt_precision, dt_recall, dt_f1]\n\n# Random Forest results\nrf_accuracy = accuracy_score(y_test, y_pred_rf)\nrf_precision = precision_score(y_test, y_pred_rf)\nrf_recall = recall_score(y_test, y_pred_rf)\nrf_f1 = f1_score(y_test, y_pred_rf)\nrf_results = [rf_accuracy, rf_precision, rf_recall, rf_f1]\n\n# SVM results\nsvm_accuracy = accuracy_score(y_test, y_pred_svm)\nsvm_precision = precision_score(y_test, y_pred_svm)\nsvm_recall = recall_score(y_test, y_pred_svm)\nsvm_f1 = f1_score(y_test, y_pred_svm)\nsvm_results = [svm_accuracy, svm_precision, svm_recall, svm_f1]\n\n# Stacking Ensemble results\nstacking_accuracy = accuracy_score(y_test, y_pred_stacking)\nstacking_precision = precision_score(y_test, y_pred_stacking)\nstacking_recall = recall_score(y_test, y_pred_stacking)\nstacking_f1 = f1_score(y_test, y_pred_stacking)\nstacking_results = [stacking_accuracy, stacking_precision, stacking_recall, stacking_f1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **CROSS-VALIDATION**","metadata":{"id":"cMQShWw635Ar"}},{"cell_type":"code","source":"# Models\ndt_model = DecisionTreeClassifier(splitter='random', min_samples_split=9, min_samples_leaf=4, max_features='sqrt', max_depth=10, criterion='entropy', random_state=42)\nsvm_model = SVC(kernel='sigmoid', gamma='scale', degree=2, coef0=0.0, C=0.01, random_state=42)\nrf_model = RandomForestClassifier(n_estimators=100, min_samples_split=7, min_samples_leaf=1, max_features='sqrt', max_depth=10, bootstrap=True, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the base estimators with updated hyperparameters\nestimators = [\n    ('dt', DecisionTreeClassifier(max_depth=30, random_state=42)),  # Updated max_depth for Decision Tree\n    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),  # Updated n_estimators and max_depth for Random Forest\n    ('svm', SVC(C=1.0, probability=True, random_state=42))  # Updated C for SVM\n]\n\n# Define the stacking model with Logistic Regression as the final estimator with updated hyperparameter\nstacking_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(C=0.1), cv=5)  # Updated C for final estimator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline  # Make sure this is from imblearn\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nX = data_imputed.drop('Outcome', axis=1)\n\n# Outcome (target)\ny = data_imputed['Outcome']\n\n# Custom Transformer for IQR-based outlier handling\nclass IQRTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, strategy='median'):\n        self.strategy = strategy\n        self.medians = None\n\n    def fit(self, X, y=None):\n        # Calculate the IQR and medians for each feature\n        Q1 = np.percentile(X, 25, axis=0)\n        Q3 = np.percentile(X, 75, axis=0)\n        IQR = Q3 - Q1\n        self.lower_bound = Q1 - 1.5 * IQR\n        self.upper_bound = Q3 + 1.5 * IQR\n        self.medians = np.median(X, axis=0)\n        return self\n\n    def transform(self, X):\n        # Replace outliers with the median\n        X_transformed = np.where((X < self.lower_bound) | (X > self.upper_bound), self.medians, X)\n        return X_transformed\n\n# Define the base models\ndt_model = DecisionTreeClassifier(splitter='best', min_samples_split=16, min_samples_leaf=9, max_features='log2', max_depth=5, criterion='gini', random_state=42)\nsvm_model = SVC(kernel='linear', gamma='auto', degree=3, coef0=0.1, C=0.1, random_state=42)\nrf_model = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features=None, max_depth=None, bootstrap=True, random_state=42)\n\n# Define the base estimators with the updated hyperparameters\nestimators = [\n    ('dt', DecisionTreeClassifier(max_depth=20, random_state=42)),  # Updated max_depth for Decision Tree\n    ('rf', RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42)),  # Updated n_estimators and max_depth for Random Forest\n    ('svm', SVC(C=0.1, probability=True, random_state=42))  # Updated C for SVM\n]\n\n# Stacking model with updated final_estimator\nstacking_model = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression(C=1.0),  # Updated C for Logistic Regression final estimator\n    cv=5\n)\n\n# Scoring metrics for cross-validation\nscoring = ['accuracy', 'precision', 'recall', 'f1']\n\n# Create pipelines with SMOTE applied within the cross-validation process\n\n# Decision Tree pipeline with SMOTE\ndt_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Median imputation for missing values\n    ('iqr', IQRTransformer()),                     # IQR outlier handling\n    ('scaler', StandardScaler()),                  # Standard scaling\n    ('smote', SMOTE(random_state=42)),                            # SMOTE applied within each fold\n    ('classifier', dt_model)                       # Decision Tree model\n])\n\n# SVM pipeline with SMOTE\nsvm_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Median imputation for missing values\n    ('iqr', IQRTransformer()),                     # IQR outlier handling\n    ('scaler', StandardScaler()),                  # Standard scaling\n    ('smote', SMOTE(random_state=42)),                            # SMOTE applied within each fold\n    ('classifier', svm_model)                      # SVM model\n])\n\n# Random Forest pipeline with SMOTE\nrf_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Median imputation for missing values\n    ('iqr', IQRTransformer()),                     # IQR outlier handling\n    ('scaler', StandardScaler()),                  # Standard scaling\n    ('smote', SMOTE(random_state=42)),                            # SMOTE applied within each fold\n    ('classifier', rf_model)                       # Random Forest model\n])\n\n# Stacking model pipeline with SMOTE\nstacking_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),  # Median imputation for missing values\n    ('iqr', IQRTransformer()),                     # IQR outlier handling\n    ('scaler', StandardScaler()),                  # Standard scaling\n    ('smote', SMOTE(random_state=42)),                            # SMOTE applied within each fold\n    ('classifier', stacking_model)                 # Stacking model\n])\n\n# Perform cross-validation with multiple scoring metrics\n\n# Cross-validation for Decision Tree\ncv_dt_results = cross_validate(dt_pipeline, X, y, cv=5, scoring=scoring)\n\n# Cross-validation for Random Forest\ncv_rf_results = cross_validate(rf_pipeline, X, y, cv=5, scoring=scoring)\n\n# Cross-validation for SVM\ncv_svm_results = cross_validate(svm_pipeline, X, y, cv=5, scoring=scoring)\n\n# Cross-validation for Stacking model\ncv_stacking_results = cross_validate(stacking_pipeline, X, y, cv=5, scoring=scoring)\n\n# Print cross-validation results for Decision Tree\nprint(\"Decision Tree Cross-Validation Results:\")\nprint(f\"Accuracy: {cv_dt_results['test_accuracy'].mean():.2f}\")\nprint(f\"Precision: {cv_dt_results['test_precision'].mean():.2f}\")\nprint(f\"Recall: {cv_dt_results['test_recall'].mean():.2f}\")\nprint(f\"F1-Score: {cv_dt_results['test_f1'].mean():.2f}\\n\")\n\n# Print cross-validation results for Random Forest\nprint(\"Random Forest Cross-Validation Results:\")\nprint(f\"Accuracy: {cv_rf_results['test_accuracy'].mean():.2f}\")\nprint(f\"Precision: {cv_rf_results['test_precision'].mean():.2f}\")\nprint(f\"Recall: {cv_rf_results['test_recall'].mean():.2f}\")\nprint(f\"F1-Score: {cv_rf_results['test_f1'].mean():.2f}\\n\")\n\n# Print cross-validation results for SVM\nprint(\"SVM Cross-Validation Results:\")\nprint(f\"Accuracy: {cv_svm_results['test_accuracy'].mean():.2f}\")\nprint(f\"Precision: {cv_svm_results['test_precision'].mean():.2f}\")\nprint(f\"Recall: {cv_svm_results['test_recall'].mean():.2f}\")\nprint(f\"F1-Score: {cv_svm_results['test_f1'].mean():.2f}\\n\")\n\n# Print cross-validation results for Stacking model\nprint(\"Stacking Model Cross-Validation Results:\")\nprint(f\"Accuracy: {cv_stacking_results['test_accuracy'].mean():.2f}\")\nprint(f\"Precision: {cv_stacking_results['test_precision'].mean():.2f}\")\nprint(f\"Recall: {cv_stacking_results['test_recall'].mean():.2f}\")\nprint(f\"F1-Score: {cv_stacking_results['test_f1'].mean():.2f}\\n\")\n\n# Decision Tree cross-validation results\ncv_dt_accuracy = cv_dt_results['test_accuracy'].mean()\ncv_dt_precision = cv_dt_results['test_precision'].mean()\ncv_dt_recall = cv_dt_results['test_recall'].mean()\ncv_dt_f1 = cv_dt_results['test_f1'].mean()\ncv_dt_metrics = [cv_dt_accuracy, cv_dt_precision, cv_dt_recall, cv_dt_f1]\n\n# Random Forest cross-validation results\ncv_rf_accuracy = cv_rf_results['test_accuracy'].mean()\ncv_rf_precision = cv_rf_results['test_precision'].mean()\ncv_rf_recall = cv_rf_results['test_recall'].mean()\ncv_rf_f1 = cv_rf_results['test_f1'].mean()\ncv_rf_metrics = [cv_rf_accuracy, cv_rf_precision, cv_rf_recall, cv_rf_f1]\n\n# SVM cross-validation results\ncv_svm_accuracy = cv_svm_results['test_accuracy'].mean()\ncv_svm_precision = cv_svm_results['test_precision'].mean()\ncv_svm_recall = cv_svm_results['test_recall'].mean()\ncv_svm_f1 = cv_svm_results['test_f1'].mean()\ncv_svm_metrics = [cv_svm_accuracy, cv_svm_precision, cv_svm_recall, cv_svm_f1]\n\n# Stacking Ensemble cross-validation results\ncv_stacking_accuracy = cv_stacking_results['test_accuracy'].mean()\ncv_stacking_precision = cv_stacking_results['test_precision'].mean()\ncv_stacking_recall = cv_stacking_results['test_recall'].mean()\ncv_stacking_f1 = cv_stacking_results['test_f1'].mean()\ncv_stacking_metrics = [cv_stacking_accuracy, cv_stacking_precision, cv_stacking_recall, cv_stacking_f1]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **EVALUATION METRICS**","metadata":{"id":"dXvqjDHI4RhE"}},{"cell_type":"code","source":"import pandas as pd\n\n# Combine all results into a single DataFrame\nresults_df = pd.DataFrame({\n    'Protocol': ['Train-Test'] * 4 + ['Cross-validation'] * 4,\n    'Algorithm': ['DT', 'RF', 'SVM', 'Stacking Ensemble'] * 2,\n    'Accuracy': [\n        dt_results[0], rf_results[0], svm_results[0], stacking_results[0],\n        cv_dt_metrics[0], cv_rf_metrics[0], cv_svm_metrics[0], cv_stacking_metrics[0]\n    ],\n    'Precision': [\n        dt_results[1], rf_results[1], svm_results[1], stacking_results[1],\n        cv_dt_metrics[1], cv_rf_metrics[1], cv_svm_metrics[1], cv_stacking_metrics[1]\n    ],\n    'Recall': [\n        dt_results[2], rf_results[2], svm_results[2], stacking_results[2],\n        cv_dt_metrics[2], cv_rf_metrics[2], cv_svm_metrics[2], cv_stacking_metrics[2]\n    ],\n    'F1-Score': [\n        dt_results[3], rf_results[3], svm_results[3], stacking_results[3],\n        cv_dt_metrics[3], cv_rf_metrics[3], cv_svm_metrics[3], cv_stacking_metrics[3]\n    ]\n})\n\n# Format the metrics to two decimal places\nresults_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']] = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score']].round(2)","metadata":{"executionInfo":{"elapsed":369,"status":"ok","timestamp":1725888959724,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"GyL3VmQXG9GT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the columns back to numeric if they were formatted as strings\nresults_df['Accuracy'] = pd.to_numeric(results_df['Accuracy'], errors='coerce')\nresults_df['Precision'] = pd.to_numeric(results_df['Precision'], errors='coerce')\nresults_df['Recall'] = pd.to_numeric(results_df['Recall'], errors='coerce')\nresults_df['F1-Score'] = pd.to_numeric(results_df['F1-Score'], errors='coerce')\n\n# Now apply the desired formatting\nresults_df['Accuracy'] = results_df['Accuracy'].apply(lambda x: f\"{x*100:.2f}\")\nresults_df['Precision'] = results_df['Precision'].apply(lambda x: f\"{x:.2f}\")\nresults_df['Recall'] = results_df['Recall'].apply(lambda x: f\"{x:.2f}\")\nresults_df['F1-Score'] = results_df['F1-Score'].apply(lambda x: f\"{x:.2f}\")\n\n# Pretty display of the table using tabulate (optional)\nfrom tabulate import tabulate\n\ntitle = \"Performance Metrics of Different Algorithms from my report\"\nprint(title)\nprint(tabulate(results_df, headers='keys', tablefmt='pretty'))","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725888960964,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"npkDuhwDG9Nm","outputId":"2e17bfe6-0d53-4a00-a596-e090c1c3e8dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **COMPARISIONS**","metadata":{}},{"cell_type":"markdown","source":"### **PERFORMANCE METRICS OF DIFFERENT ALGORITHMS FROM THE HD PAPER**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Creating a DataFrame for the table excluding Neural Networks (NN)\ndata = {\n    'Protocol': ['Train-Test', 'Train-Test', 'Train-Test', 'Train-Test', 'Cross-validation', 'Cross-validation', 'Cross-validation', 'Cross-validation'],\n    'Algorithm': ['DT', 'RF', 'SVM', 'Stacking Ensemble', 'DT', 'RF', 'SVM', 'Stacking Ensemble'],\n    'Accuracy': [65.08, 79.33, 69.03, 75.03, 68.31, 76.81, 68.61, 77.10],\n    'Precision': [.65, .80, .69, .75, .65, .77, .68, .68],\n    'Recall': [.65, .79, .69, .75, .68, .77, .70, .70],\n    'F1-Score': [.65, .79, .69, .75, .67, .78, .69, .69]\n}\n\npaper_df = pd.DataFrame(data)\n\n# Pretty display of the table using tabulate (optional)\nfrom tabulate import tabulate\n\ntitle = \"Performance Metrics of Different Algorithms from the HD paper\"\nprint(title)\nprint(tabulate(paper_df, headers='keys', tablefmt='pretty'))","metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1725888962702,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"XdruaLFq6xzP","outputId":"e387cd1e-2abd-487d-eb66-dc01da84e280"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **COMPARISON METRICS OF MODEL PERFORMANCE BASED ON EVALUATION METRICS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# DataFrames for comparison\nmy_data = results_df.copy()\n\npaper_data = {\n    'Accuracy': [65.08, 79.33, 69.03, 75.03, 68.31, 76.81, 68.61, 77.10],\n    'Precision': [0.65, 0.80, 0.69, 0.75, 0.65, 0.77, 0.68, 0.68],\n    'Recall': [0.65, 0.79, 0.69, 0.75, 0.68, 0.77, 0.70, 0.70],\n    'F1-Score': [0.65, 0.79, 0.69, 0.75, 0.67, 0.78, 0.69, 0.69]\n}\n\n# Convert to DataFrames\nmy_df = pd.DataFrame(my_data)\npaper_df = pd.DataFrame(paper_data)\n\n# Initialize a dictionary to store metrics\nmetrics = {\n    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n    'Mean Squared Error': [\n        mean_squared_error(paper_df['Accuracy'], my_df['Accuracy']),\n        mean_squared_error(paper_df['Precision'], my_df['Precision']),\n        mean_squared_error(paper_df['Recall'], my_df['Recall']),\n        mean_squared_error(paper_df['F1-Score'], my_df['F1-Score'])\n    ],\n    'Mean Absolute Error': [\n        mean_absolute_error(paper_df['Accuracy'], my_df['Accuracy']),\n        mean_absolute_error(paper_df['Precision'], my_df['Precision']),\n        mean_absolute_error(paper_df['Recall'], my_df['Recall']),\n        mean_absolute_error(paper_df['F1-Score'], my_df['F1-Score'])\n    ]\n}\n\n# Convert metrics dictionary to DataFrame\nmetrics_df = pd.DataFrame(metrics)\n\n# Pretty display of the metrics table using tabulate\nfrom tabulate import tabulate\n\ntitle = \"Comparison Metrics of Model Performance based on Evaluation Metrics\"\nprint(title)\nprint(tabulate(metrics_df, headers='keys', tablefmt='pretty', floatfmt=\".4f\"))\n","metadata":{"executionInfo":{"elapsed":372,"status":"ok","timestamp":1725889232403,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"tSJi_SN68TeJ","outputId":"1abea557-e722-4161-e220-4509ce319da2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **COMPARISON METRICS OF MODEL PERFORMANCE BASED ON DIFFERENT METHODS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Data from the HD paper\npaper_data = {\n    'Protocol': ['Train-Test', 'Train-Test', 'Train-Test', 'Train-Test', 'Cross-validation', 'Cross-validation', 'Cross-validation', 'Cross-validation'],\n    'Algorithm': ['DT', 'RF', 'SVM', 'Stacking Ensemble', 'DT', 'RF', 'SVM', 'Stacking Ensemble'],\n    'Accuracy': [65.08, 79.33, 69.03, 75.03, 68.31, 76.81, 68.61, 77.10],\n    'Precision': [0.65, 0.80, 0.69, 0.75, 0.65, 0.77, 0.68, 0.68],\n    'Recall': [0.65, 0.79, 0.69, 0.75, 0.68, 0.77, 0.70, 0.70],\n    'F1-Score': [0.65, 0.79, 0.69, 0.75, 0.67, 0.78, 0.69, 0.69]\n}\n\n# Convert to DataFrames\nmy_df = results_df\npaper_df = pd.DataFrame(paper_data)\n\n# Assuming you have columns 'Protocol' and 'Algorithm' in both DataFrames\n# Combine Protocol and Algorithm columns into one\nmy_df['Protocol_Algorithm'] = my_df['Protocol'] + ' - ' + my_df['Algorithm']\npaper_df['Protocol_Algorithm'] = paper_df['Protocol'] + ' - ' + paper_df['Algorithm']\n\n# Drop the separate Protocol and Algorithm columns\nmy_df.drop(['Protocol', 'Algorithm'], axis=1, inplace=True)\npaper_df.drop(['Protocol', 'Algorithm'], axis=1, inplace=True)\n\na = list(my_df.Protocol_Algorithm)\n\n# Transposing the dataset and setting 'Protocol_Algorithm' as the column names\nmy_df = my_df.set_index('Protocol_Algorithm').T\npaper_df = paper_df.set_index('Protocol_Algorithm').T\n\n# Initialize a dictionary to store metrics\nmetrics = {\n    'Metric': a,\n    'Mean Squared Error': [\n        mean_squared_error(paper_df['Train-Test - DT'], my_df['Train-Test - DT']),\n        mean_squared_error(paper_df['Train-Test - RF'], my_df['Train-Test - RF']),\n        mean_squared_error(paper_df['Train-Test - SVM'], my_df['Train-Test - SVM']),\n        mean_squared_error(paper_df['Train-Test - Stacking Ensemble'], my_df['Train-Test - Stacking Ensemble']),\n        mean_squared_error(paper_df['Cross-validation - DT'], my_df['Cross-validation - DT']),\n        mean_squared_error(paper_df['Cross-validation - RF'], my_df['Cross-validation - RF']),\n        mean_squared_error(paper_df['Cross-validation - SVM'], my_df['Cross-validation - SVM']),\n        mean_squared_error(paper_df['Cross-validation - Stacking Ensemble'], my_df['Cross-validation - Stacking Ensemble'])\n    ],\n    'Mean Absolute Error': [\n        mean_absolute_error(paper_df['Train-Test - DT'], my_df['Train-Test - DT']),\n        mean_absolute_error(paper_df['Train-Test - RF'], my_df['Train-Test - RF']),\n        mean_absolute_error(paper_df['Train-Test - SVM'], my_df['Train-Test - SVM']),\n        mean_absolute_error(paper_df['Train-Test - Stacking Ensemble'], my_df['Train-Test - Stacking Ensemble']),\n        mean_absolute_error(paper_df['Cross-validation - DT'], my_df['Cross-validation - DT']),\n        mean_absolute_error(paper_df['Cross-validation - RF'], my_df['Cross-validation - RF']),\n        mean_absolute_error(paper_df['Cross-validation - SVM'], my_df['Cross-validation - SVM']),\n        mean_absolute_error(paper_df['Cross-validation - Stacking Ensemble'], my_df['Cross-validation - Stacking Ensemble'])\n    ]\n}\n\n# Convert metrics dictionary to DataFrame\nmetrics_df = pd.DataFrame(metrics)\nmetrics_df.rename(columns={'Metric': 'Model'}, inplace=True)\n# Pretty display of the metrics table using tabulate\nfrom tabulate import tabulate\n\ntitle = \"Comparison Metrics of Model Performance based on Different Methods\"\nprint(title)\nprint(tabulate(metrics_df, headers='keys', tablefmt='pretty', floatfmt=\".4f\"))","metadata":{"executionInfo":{"elapsed":373,"status":"ok","timestamp":1725891222083,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"pyLG537tEDJT","outputId":"8be2b32a-bdb9-4584-fa42-53baa6c2989d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tabulate import tabulate\n\n# Convert the dictionary 'paper_data' into a DataFrame\npaper_df = pd.DataFrame(paper_data)\n\n# Combine 'Protocol' and 'Algorithm' columns into a single 'Protocol_Algorithm' column\n# Then drop the original 'Protocol' and 'Algorithm' columns from the DataFrame\npaper_df['Protocol_Algorithm'] = paper_df['Protocol'] + ' - ' + paper_df['Algorithm']\npaper_df = paper_df.drop(columns=['Protocol', 'Algorithm'])\n\n# Transpose the DataFrame for easier comparison later (switching rows and columns)\npaper_df_t = paper_df.T\n\n# Set the 5th row (which is now row index 4 due to 0-based indexing) as the column headers\npaper_df_t.columns = paper_df_t.iloc[4]\n\n# Drop the 5th row (index 4) now that it has been used to set column headers\npaper_df_t = paper_df_t.drop(paper_df_t.index[4])\n\n# Ensure both DataFrames only contain numeric columns by converting all columns to numeric\n# 'errors=coerce' ensures that non-numeric values are replaced with NaN\npaper_df_t_numeric = paper_df_t.apply(pd.to_numeric, errors='coerce')\nmy_df_numeric = my_df.apply(pd.to_numeric, errors='coerce')\n\n# Calculate the percentage difference between 'my_df_numeric' and 'paper_df_t_numeric'\n# This formula calculates the absolute percentage difference\npercentage_difference_df = ((my_df_numeric - paper_df_t_numeric) / paper_df_t_numeric).abs() * 100\n\n# Format the values to include a percentage sign, rounding to two decimal places\npercentage_difference_df = percentage_difference_df.map(lambda x: f'{x:.2f}%')\n\n# Transpose the result back to the original format (optional, if needed for final display)\npercentage_difference_df = percentage_difference_df.T\n\n# Display the table in a pretty format using the tabulate library\ntitle = \"Comparison of Percentage Differences in Model Performance Metrics Across Protocols and Algorithms\"\nprint(title)\nprint(tabulate(percentage_difference_df, headers='keys', tablefmt='pretty', floatfmt=\".4f\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **QUESTION 2**","metadata":{"id":"p262gt661I7I"}},{"cell_type":"markdown","source":"## **DATA LOADING**","metadata":{"id":"fb8QS6fCsk_w"}},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nfrom sklearn.pipeline import Pipeline  # Use scikit-learn's Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, FunctionTransformer\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, VarianceThreshold\n\n# # Load the PIMA Indians diabetes dataset\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\ncolumns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n           'DiabetesPedigreeFunction', 'Age', 'Outcome']\n\n# # Load the dataset into a DataFrame\ndata = pd.read_csv(url, names=columns)","metadata":{"executionInfo":{"elapsed":653,"status":"ok","timestamp":1725880171519,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"FHEEaPmKsk_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **VISUALISATIONS**","metadata":{"id":"DS1KpmP4sk_x"}},{"cell_type":"markdown","source":"### **HISTOGRAM**","metadata":{}},{"cell_type":"code","source":"# Set the number of features and extract the 'crest' color palette\nnum_features = data.drop('Outcome', axis=1).shape[1]\ncrest_colors = sns.color_palette(\"crest\", num_features)\n\n# Set a larger figure size for the plot grid\nplt.figure(figsize=(8, 7))\n\n# Loop through each feature and create histograms with 'crest' colors\nfor i, column in enumerate(data.drop('Outcome', axis=1).columns):\n    plt.subplot(3, 3, i + 1)\n    plt.hist(data[column], bins=20, color=crest_colors[i], edgecolor='black', linewidth=1.2, alpha=0.75)\n    plt.title(column)\n\n# Customizing the layout and adding a title\nplt.suptitle('Feature Distributions in PIMA Indians Diabetes Dataset', fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust the layout to make space for the title\n\n# Display the histograms\nplt.show()","metadata":{"executionInfo":{"elapsed":2735,"status":"ok","timestamp":1725880174249,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"Cfxh8xNYsk_x","outputId":"29cdb94b-a098-4154-a373-c46f96d5884a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **PIE PLOT**","metadata":{}},{"cell_type":"code","source":"# Count the frequency of each outcome (0 = No Diabetes, 1 = Diabetes)\noutcome_counts = data['Outcome'].value_counts()\n\n# Set the color from the \"crest\" palette for the pie chart\ncrest_colors = sns.color_palette(\"crest\", as_cmap=False)\n\n# Create the pie chart\nplt.figure(figsize=(6, 6))\nplt.pie(outcome_counts, labels=['No Diabetes', 'Diabetes'], autopct='%1.1f%%', startangle=90, colors=crest_colors, wedgeprops={'edgecolor': 'black'})\n\n# Add a title\nplt.title('Distribution of Diabetes Outcome', fontsize=16)\n\n# Display the pie chart\nplt.show()","metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1725880174249,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"cJwmjNXTsk_x","outputId":"cc2e9807-9c77-4a43-b2ca-4844b586f4ff"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **PAIR PLOT**","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Suppress warnings globally\nwarnings.filterwarnings('ignore')\n\n# Assuming 'data' is already defined and contains your dataset\np = sns.pairplot(data, hue='Outcome', palette='crest')\n\n# Add title and adjust layout\np.fig.suptitle('Pairplot with Crest Color Palette', y=1.02)\np.fig.tight_layout()\n\nplt.show()\n","metadata":{"executionInfo":{"elapsed":32843,"status":"ok","timestamp":1725880207070,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"LeonPvDOsk_x","outputId":"ef6d94f5-24ee-4216-ef9b-59ebeee55655"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CORRELATION HEATMAP**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 7))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(data.corr(), cmap ='crest', annot=True, fmt=\".2f\")  # seaborn has very simple solution for heatmap","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1725880207070,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"uhXUI_6-sk_x","outputId":"6017eecd-12f6-4264-ec42-898764d29adb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **BOX PLOT**","metadata":{}},{"cell_type":"code","source":"# Plot box plots with the 'crest' palette\nplt.figure(figsize=(8, 6))\n\n# Create a single boxplot for all features\nsns.boxplot(data=data, palette=\"crest\")\nplt.title('Boxplot of Features with Crest Palette')\n\n# Rotate the x-tick labels to avoid overlap\nplt.xticks(rotation=90)\nplt.grid()\nplt.tight_layout()\nplt.show()","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725880207070,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"eV6Vmx_psk_x","outputId":"56803089-29ad-4b47-ef61-0b119dc69e84"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **PREPROCESSING**","metadata":{"id":"4DatzrVtsk_x"}},{"cell_type":"code","source":"from imblearn.pipeline import Pipeline as ImbPipeline\n\n# Replace zero values with NaN for relevant columns\ndata[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = data[\n    ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)\n\n# Apply SimpleImputer outside the pipeline to handle missing values\nimputer = SimpleImputer(strategy='median')\nX = data.drop('Outcome', axis=1)\ny = data['Outcome']\n\n# Fit and transform the data with SimpleImputer\nX_imputed = imputer.fit_transform(X)\n\n# Convert the imputed data back to a DataFrame\nX_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n\n# Function to detect and cap outliers using IQR method\ndef cap_outliers(X):\n    X = pd.DataFrame(X, columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age'])\n    for column in ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age']:\n        Q1 = X[column].quantile(0.25)\n        Q3 = X[column].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        X[column] = np.where(X[column] < lower_bound, lower_bound, X[column])\n        X[column] = np.where(X[column] > upper_bound, upper_bound, X[column])\n    return X\n\n# Apply log transformation for skewed features\ndef log_transform(X):\n    X = pd.DataFrame(X, columns=['Insulin', 'SkinThickness', 'BMI'])\n    for column in X.columns:\n        X[column] = np.log1p(X[column])\n    return X\n\n# Define the preprocessing steps without 'selecting' inside the ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('outlier_capping', FunctionTransformer(cap_outliers), ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'Age']),\n        ('log_transform', FunctionTransformer(log_transform), ['Insulin', 'SkinThickness', 'BMI']),\n        ('poly', PolynomialFeatures(3), ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']),\n        ('scaling', StandardScaler(), ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\n    ]\n)\n\n# Define the updated model pipeline with SelectKBest after preprocessor\npipeline = ImbPipeline(steps=[\n    ('preprocessor', preprocessor),  # Data preprocessing steps\n    ('variance_threshold', VarianceThreshold(threshold=0)),\n    ('selecting', SelectKBest(f_classif, k=20)),  # Feature selection with SelectKBest after preprocessing\n    ('model', StackingClassifier(\n        estimators=[\n            ('dt', DecisionTreeClassifier(random_state=42)),\n            ('rf', RandomForestClassifier(random_state=42)),\n            ('xgb', XGBClassifier(random_state=42))\n        ],\n        final_estimator=LogisticRegression(),\n        n_jobs=-1\n    ))\n])\n\n# Split the imputed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.3, random_state=42)\n\n# Apply RandomOverSampler separately\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1725880207071,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"-N28O7xZH9bW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **PREDICTIONS ON DIFFERENT MODELS**","metadata":{"id":"zQS47xgNsk_x"}},{"cell_type":"code","source":"# !pip install lazypredict\nimport lightgbm as lgb\nfrom lazypredict.Supervised import LazyClassifier\nfrom tabulate import tabulate\nimport re\n\n# Custom logger to suppress LightGBM logs\nclass DummyLogger:\n    def __init__(self):\n        pass\n\n    def info(self, msg):\n        pass  # Suppress info messages\n\n    def warning(self, msg):\n        pass  # Suppress warning messages\n\n# Register the dummy logger to suppress LightGBM logs\nlgb.register_logger(DummyLogger())\n\n# Assuming X_train, X_test, y_train, y_test are already defined\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)\n\n# Convert models DataFrame to a tabulated format\nmodels_table = tabulate(models, headers='keys', tablefmt='grid')\n\n# Save the models result to a text file\nwith open('models_result.txt', 'w') as file:\n    file.write(models_table)\n\n# Read the file content\nwith open('models_result.txt', 'r') as file:\n    content = file.read()\n\n# Remove LightGBM-specific logs using regex (in case any slip through)\nfiltered_content = re.sub(r'\\[LightGBM\\][^\\n]*\\n', '', content)\n\n# Save the filtered content into a new file\nwith open('filtered_models_result.txt', 'w') as file:\n    file.write(filtered_content)\n\n# Output the filtered content\nprint(filtered_content)\n","metadata":{"executionInfo":{"elapsed":11729,"status":"ok","timestamp":1725880226662,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"CbTxDq_2sk_x","outputId":"571f409f-3742-42ab-e6ee-64ac92ac2933"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **CROSS-VALIDATION WITH RANDOMIZEDSEARCHCV**","metadata":{"id":"pq0hmoRUsk_y"}},{"cell_type":"code","source":"# Define the parameter grid\nparam_grid = {\n    # DecisionTreeClassifier parameters\n    'model__dt__max_depth': [10, 20, 30],  # Maximum depth of the decision tree\n    'model__dt__min_samples_split': [5, 10],  # Minimum samples required to split a node\n    'model__dt__min_samples_leaf': [2, 5],  # Minimum number of samples at a leaf node\n\n    # RandomForestClassifier parameters\n    'model__rf__n_estimators': [100, 200],  # Number of trees in the forest\n    'model__rf__max_depth': [10, 20],  # Maximum depth of the trees\n    'model__rf__min_samples_split': [5, 10],  # Minimum samples required to split a node\n\n    # XGBClassifier parameters\n    'model__xgb__n_estimators': [100, 200],  # Number of boosting rounds\n    'model__xgb__learning_rate': [0.1, 0.2, 0.3, 0.5],  # Step size shrinkage used in updates to prevent overfitting\n    'model__xgb__max_depth': [5, 7],  # Maximum depth of the trees\n\n    # LogisticRegression (final estimator in stacking) parameters\n    'model__final_estimator__C': [0.1, 1, 10],  # Inverse regularization strength\n    'model__final_estimator__solver': ['lbfgs'],  # Algorithm to use for optimization\n    'model__final_estimator__penalty': ['l2'],  # Type of regularization to apply\n    'model__final_estimator__max_iter': [100, 150, 200],  # Maximum number of iterations for the solver\n\n    # SelectKBest parameters (feature selection)\n    'selecting__k': [10, 20, 30, 40, 50],  # Number of features to select\n\n    # PolynomialFeatures parameters (preprocessing step)\n    'preprocessor__poly__degree': [2, 3, 4],  # Degree of the polynomial features to be generated\n}\n\n\n# Set up RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, n_iter=10,\n                                   scoring='accuracy', cv=5, random_state=42, n_jobs=-1)\n\n# Perform the randomized search on resampled data\nrandom_search.fit(X_resampled, y_resampled)","metadata":{"executionInfo":{"elapsed":299120,"status":"ok","timestamp":1725880525780,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"VYu_kV7Fsk_y","outputId":"84f330a5-bced-420c-83df-0deb72eed115"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **EVALUATION METRICS**","metadata":{"id":"LlCnBVU-sk_y"}},{"cell_type":"code","source":"# Get the best parameters and display them\nbest_params = random_search.best_params_\nprint(f\"Best Parameters: {best_params}\")\n\n# Make predictions using the best estimator on the test set\ny_pred_test = random_search.best_estimator_.predict(X_test)\n\n# Generate and print the classification report for the tuned model on the test set\nreport_test = classification_report(y_test, y_pred_test)\nprint(\"\\nClassification Report (Test Set):\\n\", report_test)","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1725880525780,"user":{"displayName":"Hai Nam Le","userId":"01287890343435723484"},"user_tz":-600},"id":"VW4Zpd4Vsk_y","outputId":"218f0b1b-5c55-46d4-da95-efc879df00f1"},"execution_count":null,"outputs":[]}]}