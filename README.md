# âœ¨ Machine Learning Approaches for Early Detection of Diabetes

## ğŸŒŸ A Comparative Study and Novel Solution on the PIMA Indians Diabetes Dataset
<img src="assets/pipeline.png" alt="Model Architecture" style="border: 5px solid black;"/>

**Author:** Hai Nam LE

---

## ğŸ”§ Dependencies

<div align="left">
  <a href="https://lightgbm.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/lightgbm-4.5.0-blue?style=flat-square" alt="lightgbm"></a>
  <a href="https://pypi.org/project/lazypredict/"><img src="https://img.shields.io/badge/lazypredict-0.2.11-blue?style=flat-square" alt="lazypredict"></a>
  <a href="https://pypi.org/project/tabulate/"><img src="https://img.shields.io/badge/tabulate-0.9.0-blue?style=flat-square" alt="tabulate"></a>
  <a href="https://imbalanced-learn.org/stable/"><img src="https://img.shields.io/badge/imbalanced--learn-0.12.4-blue?style=flat-square" alt="imbalanced-learn"></a>
  <a href="https://scikit-learn.org/stable/"><img src="https://img.shields.io/badge/scikit--learn-1.3.0-blue?style=flat-square" alt="scikit-learn"></a>
  <a href="https://pandas.pydata.org/"><img src="https://img.shields.io/badge/pandas-2.1.0-blue?style=flat-square" alt="pandas"></a>
  <a href="https://numpy.org/"><img src="https://img.shields.io/badge/numpy-1.24.3-blue?style=flat-square" alt="numpy"></a>
  <a href="https://xgboost.readthedocs.io/en/stable/"><img src="https://img.shields.io/badge/xgboost-1.7.5-blue?style=flat-square" alt="xgboost"></a>
</div>

---

## ğŸ“‘ Table of Contents

- [ğŸ“œ Project Overview](#-project-overview)
- [ğŸ“Š Dataset Summary](#-dataset-summary)
- [âš™ï¸ Installation](#%EF%B8%8F-installation)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ› ï¸ Preprocessing Steps](#%EF%B8%8F-preprocessing-steps)
- [ğŸ“š Model Description](#-model-description)
- [ğŸ“‹ Experimental Protocol](#-experimental-protocol)
- [ğŸ“ˆ Evaluation Metrics](#-evaluation-metrics)
- [ğŸ† Results](#-results)
  - [ğŸ“Œ Model Performance](#-model-performance)
  - [ğŸ“Š Comparison of Percentage Differences](#-comparison-of-percentage-differences)
  - [ğŸ“‰ Evaluation Metrics Comparison](#-evaluation-metrics-comparison)
  - [âš¡ LazyPredict Model Comparison](#-lazypredict-model-comparison)
- [ğŸ”„ Reproduction of Results](#-reproduction-of-results)
  - [ğŸ“ Experimental Procedure](#-experimental-procedure)
  - [ğŸ“– Comprehensive Analysis](#-comprehensive-analysis)
- [ğŸ“– Comparison with Existing Literature](#-comparison-with-existing-literature)
  - [ğŸ”‘ Key Observations](#-key-observations)
  - [ğŸ¯ Focus on Recall for Imbalanced Data](#-focus-on-recall-for-imbalanced-data)
  - [ğŸ§  Final Thoughts](#-final-thoughts)
- [ğŸ“š References](#-references)
- [ğŸ“§ Contact](#-contact)

---

## ğŸ“œ Project Overview

This project explores various machine learning approaches for the early detection of diabetes using the **PIMA Indians Diabetes Dataset**. It conducts a comparative study of different classifiers and introduces a novel stacking ensemble method to enhance predictive performance. The study addresses challenges such as missing values, outliers, and class imbalance through robust preprocessing techniques.

---

## ğŸ“Š Dataset Summary

- **Source:** [Kaggle Repository](https://www.kaggle.com/uciml/pima-indians-diabetes-database)
- **Original Provider:** National Institute of Diabetes and Digestive and Kidney Diseases
- **Number of Instances:** 768 female patients
- **Positive Cases (Diabetic):** 268 (35%)
- **Negative Cases (Non-Diabetic):** 500 (65%)

### Features

1. ğŸ¤° Number of pregnancies
2. ğŸ©¸ Plasma glucose concentration
3. ğŸ©º Diastolic blood pressure
4. ğŸ›‘ Triceps skinfold thickness
5. ğŸ§ª Serum insulin levels
6. âš–ï¸ Body mass index (BMI)
7. ğŸ“ˆ Diabetes pedigree function
8. ğŸ•’ Age

### Challenges

- **â— Missing Values:** Represented by zeroes in some features (Insulin, SkinThickness, etc.)
- **âš ï¸ Outliers:** Presence of outliers across multiple features

---

## âš™ï¸ Installation

### Prerequisites

- **ğŸ Python:** Version 3.7 or higher
- **ğŸ“¦ pip:** Python package manager

### Clone the Repository

```bash
git clone https://github.com/tomledeakin/Diabetes-Detection-ML-PIMA.git
cd Diabetes-Detection-ML-PIMA
```
---
## ğŸ› ï¸ Setting Up the Environment

### Create a Virtual Environment (Optional but Recommended)
Creating a virtual environment helps manage dependencies and avoid conflicts.

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Install Required Dependencies

Install the necessary Python libraries using `pip`.

If a `requirements.txt` file is provided:
```bash
pip install -r requirements.txt
```
---
## ğŸ“‚ Project Structure
```bash
Diabetes-Detection-ML-PIMA/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ PIMA_indians_diabetes.csv
â”œâ”€â”€ pima-dataset.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ models_result.txt
â””â”€â”€ requirements.txt
```

### Description:
- ğŸ“ **`data/`**: Contains the dataset file `PIMA_indians_diabetes.csv`.
- ğŸ““ **`pima-dataset.ipynb`**: Jupyter Notebook with the complete implementation of the project, including data preprocessing, model training, and evaluation.
- ğŸ“„ **`README.md`**: Documentation for the project.
- ğŸ“‹ **`requirements.txt`**: File specifying the Python dependencies needed to run the project.

---
## ğŸ› ï¸ Preprocessing Steps

1. ğŸ“ **Handling Zero Values**  
   - Replace zeroes in critical features (`Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`) with `NaN`.

2. ğŸ“¥ **Missing Value Imputation**  
   - Use **Median Imputation** with `SimpleImputer` to fill missing values.

3. ğŸ“Š **Outlier Detection and Capping**  
   - Apply the **Interquartile Range (IQR)** method to cap outliers beyond `1.5 * IQR`.

4. ğŸ“ **Log Transformation**  
   - Perform `log1p` transformation on skewed features (`Insulin`, `SkinThickness`, `BMI`) to reduce skewness.

5. ğŸ“ **Feature Scaling**  
   - Standardize features using `StandardScaler` to achieve a mean of 0 and variance of 1.

6. ğŸ” **Feature Engineering**  
   - Generate **polynomial features** of degree 2 to capture interactions between features.

7. ğŸ“Š **Feature Selection**  
   - Use `SelectKBest` with **ANOVA F-values** to retain the top 40 features.

8. âš–ï¸ **Class Imbalance Handling**  
   - Apply **Synthetic Minority Over-sampling Technique (SMOTE)** to balance the class distribution.

---

## ğŸ“š Model Description

### ğŸ¤– Stacking Ensemble Classifier

#### Base Classifiers
- ğŸŒ³ **DecisionTreeClassifier**
- ğŸŒ² **RandomForestClassifier**
- ğŸ“ˆ **XGBClassifier**

#### Final Estimator
- ğŸ”— **LogisticRegression** with L2 regularization

### âš™ï¸ Hyperparameter Tuning
- Utilize `RandomizedSearchCV` to optimize:
  - Hyperparameters for each base classifier
  - Polynomial feature generation
  - Feature selection parameters
- Optimization is based on cross-validated accuracy.
---
## ğŸ§ª Experimental Protocol

1. ğŸ“‚ **Data Splitting**  
   - Split the dataset into **70% training** and **30% testing**.

2. ğŸ› ï¸ **Preprocessing**  
   - Apply preprocessing steps to the training set.
   - Transform the test set using the same preprocessing pipeline.

3. ğŸ¤– **Model Training**  
   - Train models using the training set within a predefined pipeline.

4. ğŸ“Š **Evaluation**  
   - Evaluate models using:
     - ğŸ§ª **Train-Test Split**
     - ğŸ”„ **5-Fold Cross-Validation**

5. ğŸ” **Comparison**  
   - Compare reproduced results with those from the original research article using:
     - ğŸ“‰ **Mean Squared Error (MSE)**
     - ğŸ“ˆ **Mean Absolute Error (MAE)**

---

## ğŸ“ˆ Evaluation Metrics

1. ğŸ¯ **Accuracy**  
   - Measures the overall correctness of the model.

2. ğŸ” **Precision**  
   - Ratio of correctly predicted positive instances to the total predicted positives.

3. ğŸ©º **Recall (Sensitivity)**  
   - Ratio of correctly predicted positive instances to the actual positives.

4. âš–ï¸ **F1-Score**  
   - Harmonic mean of Precision and Recall, providing a balance between the two.

5. ğŸ“‰ **Mean Squared Error (MSE)**  
   - Average squared difference between the predicted and actual values.

6. ğŸ“Š **Mean Absolute Error (MAE)**  
   - Average absolute difference between the predicted and actual values.


---
## ğŸ“ˆ Results

### ğŸ† Model Performance

#### ğŸ“Š Train-Test Split
| Algorithm             | ğŸ¯ Accuracy | ğŸ” Precision | ğŸ©º Recall | âš–ï¸ F1-Score |
|-----------------------|-------------|--------------|-----------|-------------|
| Decision Tree (DT)    | 66.00%      | 0.67         | 0.62      | 0.65        |
| Random Forest (RF)    | 79.00%      | 0.78         | 0.81      | 0.79        |
| SVM                   | 70.00%      | 0.71         | 0.67      | 0.69        |
| Stacking Ensemble     | 75.00%      | 0.74         | 0.78      | 0.76        |

#### ğŸ”„ Cross-Validation
| Algorithm             | ğŸ¯ Accuracy | ğŸ” Precision | ğŸ©º Recall | âš–ï¸ F1-Score |
|-----------------------|-------------|--------------|-----------|-------------|
| Decision Tree (DT)    | 71.00%      | 0.58         | 0.69      | 0.63        |
| Random Forest (RF)    | 76.00%      | 0.64         | 0.68      | 0.66        |
| SVM                   | 75.00%      | 0.63         | 0.69      | 0.66        |
| Stacking Ensemble     | 76.00%      | 0.66         | 0.64      | 0.65        |



### ğŸ“Š Comparison of Percentage Differences

#### ğŸ“ˆ Train-Test Split
| Algorithm             | ğŸ¯ Accuracy | ğŸ” Precision | ğŸ©º Recall | âš–ï¸ F1-Score |
|-----------------------|-------------|--------------|-----------|-------------|
| Decision Tree (DT)    | 1.41%       | 3.08%        | 4.62%     | 0.00%       |
| Random Forest (RF)    | 0.42%       | 2.50%        | 2.53%     | 0.00%       |
| SVM                   | 1.41%       | 2.90%        | 2.90%     | 0.00%       |
| Stacking Ensemble     | 0.04%       | 1.33%        | 4.00%     | 1.33%       |

#### ğŸ”„ Cross-Validation
| Algorithm             | ğŸ¯ Accuracy | ğŸ” Precision | ğŸ©º Recall | âš–ï¸ F1-Score |
|-----------------------|-------------|--------------|-----------|-------------|
| Decision Tree (DT)    | 3.94%       | 10.77%       | 1.47%     | 5.97%       |
| Random Forest (RF)    | 1.05%       | 16.88%       | 11.69%    | 15.38%      |
| SVM                   | 9.31%       | 7.35%        | 1.43%     | 4.35%       |
| Stacking Ensemble     | 1.43%       | 2.94%        | 8.57%     | 5.80%       |

---

### ğŸ“‰ Evaluation Metrics Comparison

| ğŸ“ Metric    | ğŸ“‰ MSE      | ğŸ“Š MAE      |
|--------------|-------------|-------------|
| ğŸ¯ Accuracy  | 6.4789      | 1.6550      |
| ğŸ” Precision | 0.00325     | 0.0425      |
| ğŸ©º Recall    | 0.00181     | 0.03375     |
| âš–ï¸ F1-Score  | 0.00232     | 0.0300      |

---

## âš¡ LazyPredict Model Comparison


| ğŸ·ï¸ Model                       | ğŸ¯ Accuracy | âš–ï¸ Balanced Accuracy | ğŸ“ˆ ROC AUC | âš–ï¸ F1 Score | ğŸ•’ Time Taken (s) |
|--------------------------------|-------------|-----------------------|-----------|-------------|------------------|
| ğŸŒ² RandomForestClassifier      | 0.7532      | 0.7349                | 0.7349    | 0.7549      | 0.4079           |
| ğŸ’¡ LGBMClassifier              | 0.7403      | 0.7308                | 0.7308    | 0.7439      | 0.4474           |
| ğŸ§© BaggingClassifier           | 0.7576      | 0.7264                | 0.7264    | 0.7561      | 0.1091           |
| ğŸ“Š XGBClassifier               | 0.7316      | 0.7212                | 0.7212    | 0.7354      | 0.8347           |
| ğŸ§ª GaussianNB                  | 0.7403      | 0.7190                | 0.7190    | 0.7417      | 0.0314           |
| ğŸ§¬ QuadraticDiscriminantAnalysis| 0.7489      | 0.7110                | 0.7110    | 0.7455      | 0.0413           |
| âœ… CalibratedClassifierCV      | 0.7446      | 0.7077                | 0.7077    | 0.7416      | 0.0838           |
| ğŸ“ RidgeClassifier             | 0.7446      | 0.7077                | 0.7077    | 0.7416      | 0.0364           |
| ğŸ”º AdaBoostClassifier          | 0.7273      | 0.7062                | 0.7062    | 0.7291      | 0.2764           |
| âš™ï¸ SVC                         | 0.7446      | 0.7047                | 0.7047    | 0.7407      | 0.0528           |
| ğŸ“ LinearSVC                   | 0.7403      | 0.7043                | 0.7043    | 0.7377      | 0.0266           |
| ğŸ§® LinearDiscriminantAnalysis  | 0.7403      | 0.7043                | 0.7043    | 0.7377      | 0.0323           |
| ğŸ§¾ LogisticRegression          | 0.7403      | 0.7043                | 0.7043    | 0.7377      | 0.0311           |
| ğŸ“Š RidgeClassifierCV           | 0.7403      | 0.7014                | 0.7014    | 0.7368      | 0.0307           |
| ğŸŒ³ ExtraTreesClassifier         | 0.7316      | 0.7007                | 0.7007    | 0.7308      | 0.3774           |
| ğŸ”„ NuSVC                       | 0.7359      | 0.6922                | 0.6922    | 0.7308      | 0.0458           |
| ğŸ“¡ KNeighborsClassifier        | 0.7056      | 0.6896                | 0.6896    | 0.7092      | 0.0971           |
| ğŸ’» SGDClassifier               | 0.7013      | 0.6834                | 0.6834    | 0.7047      | 0.0316           |
| ğŸ§² NearestCentroid             | 0.6840      | 0.6760                | 0.6760    | 0.6897      | 0.0281           |
| ğŸŒ´ ExtraTreeClassifier          | 0.7056      | 0.6749                | 0.6749    | 0.7056      | 0.0296           |
| ğŸŒ³ DecisionTreeClassifier       | 0.6970      | 0.6742                | 0.6742    | 0.6994      | 0.0331           |
| ğŸ§® BernoulliNB                 | 0.6753      | 0.6635                | 0.6635    | 0.6808      | 0.0342           |
| ğŸ”Œ Perceptron                  | 0.6970      | 0.6624                | 0.6624    | 0.6960      | 0.0269           |
| ğŸ”— LabelPropagation            | 0.6537      | 0.6264                | 0.6264    | 0.6564      | 0.0581           |
| ğŸ“¡ LabelSpreading              | 0.6537      | 0.6264                | 0.6264    | 0.6564      | 0.0555           |
| ğŸš€ PassiveAggressiveClassifier | 0.6234      | 0.6002                | 0.6002    | 0.6284      | 0.0281           |
| ğŸŒ€ DummyClassifier             | 0.6537      | 0.5000                | 0.5000    | 0.5168      | 0.0263           |





---
## ğŸ”„ Reproduction of Results

### ğŸ§ª Experimental Procedure

1. ğŸ”§ **Fine Tuning**  
   - Test different hyperparameters to align model performance with the original research article.

2. ğŸ“‚ **Model Training for Train-Test Split**  
   - Split the data into **70% training** and **30% testing**.
   - Apply the following preprocessing steps:
     - âš™ï¸ Outlier handling
     - ğŸ“¥ Missing value imputation
     - ğŸ“ Feature scaling
     - âš–ï¸ Class imbalance handling using **SMOTE**
   - Train models and evaluate their performance on the test set.

3. ğŸ”„ **Model Training for Cross-Validation**  
   - Use the same preprocessing pipeline.
   - Perform **5-fold cross-validation** to ensure generalizability of the models.

4. ğŸ“Š **Comparison**  
   - Compare reproduced results with those from the research article using:
     - ğŸ“‰ **Mean Squared Error (MSE)**
     - ğŸ“ˆ **Mean Absolute Error (MAE)**



### ğŸ“– Comprehensive Analysis

1. ğŸŒ² **Random Forest (Train-Test)**  
   - Achieved the highest accuracy of **79%**, making it the top performer in the Train-Test evaluation.

2. ğŸ¤– **Stacking Ensemble (Cross-Validation)**  
   - Balanced performance with an accuracy of **76%**, demonstrating its strength in generalizability.

3. âš ï¸ **Discrepancies**  
   - Differences in reproduced results and the research article are attributed to:
     - Variations in hyperparameter tuning.
     - Potential **data leakage** caused by applying **SMOTE** before data splitting.

---

## ğŸ“š Comparison with Existing Literature

### ğŸ”‘ Key Observations

1. ğŸ¯ **Accuracy**  
   - The reproduced model achieved **77% accuracy**, slightly lower than the research article's reported **79%**.  
   - The difference is likely due to avoiding **data leakage**, ensuring more realistic performance estimates.

2. ğŸ©º **Recall for Imbalanced Data**  
   - Focused on recall for the diabetic class, achieving **72%**, which is crucial for early detection and intervention in diabetes cases.

3. âš–ï¸ **Model Robustness**  
   - Ensured realistic performance estimates by adhering to best practices in data splitting and preprocessing.

---

### ğŸ¯ Focus on Recall for Imbalanced Data

- **Accuracy** alone can be misleading for imbalanced datasets.  
- Emphasizing **recall** ensures that diabetic cases are correctly identified, which is vital for early intervention and treatment.  
- A high recall demonstrates the model's ability to minimize false negatives, making it a more effective tool for medical diagnostics.

---

### ğŸ§  Final Thoughts

- The reproduced model exhibits robust performance, with high recall and balanced metrics, making it suitable for the task of **early diabetes detection**.  
- While the original research achieved slightly higher accuracy, their methodology likely introduced **data leakage** during preprocessing.  
- The reproduced results are therefore more trustworthy and **generalizable** for real-world applications.

---

## ğŸ“š References

1. Reza, M.S., Amin, R., Yasmin, R., Kulsum, W., & Ruhi, S. (2024). *Improving diabetes disease patients classification using stacking ensemble method with PIMA and local healthcare data*. Heliyon, 10(2), e24536.  
2. pandas development team (2024). *pandas: Python Data Analysis Library*.  
3. NumPy Developers (2024). *NumPy*.  
4. scikit-learn Developers (2024). *scikit-learn: Machine Learning in Python*.  
5. Matplotlib Developers (2024). *Matplotlib: Visualization with Python*.  
6. Waskom, M. L. (2024). *seaborn: Statistical Data Visualization*.  
7. imbalanced-learn Developers (2024). *imbalanced-learn: A Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning*.  
8. LazyPredict Developers (2024). *LazyPredict: A Tool for Quick Model Training and Evaluation*.  
9. Microsoft (2024). *LightGBM: A Fast, Distributed, High-Performance Gradient Boosting Framework*.  

---

## ğŸ“§ Contact

For any questions or suggestions, please contact **Hai Nam LE** at:  
ğŸ“§ **Email:** [tomledeakin@gmail.com](mailto:tomledeakin@gmail.com)
